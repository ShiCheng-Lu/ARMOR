{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "import math\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import draw_bounding_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda NVIDIA GeForce GTX 1080 Ti\n",
      "1.12.1+cu116\n",
      "0.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(device)\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MusicSheetDataSet\n",
    "import random\n",
    "\n",
    "def area(box):\n",
    "    return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "def crop(image, target, region, dataset):\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for annotation in target:\n",
    "        # print(annotation)\n",
    "        orig_box = [x for x in annotation['a_bbox']]\n",
    "        new_box = [\n",
    "            max(orig_box[0], region[0]) - region[0],\n",
    "            max(orig_box[1], region[1]) - region[1],\n",
    "            min(orig_box[2], region[2]) - region[0],\n",
    "            min(orig_box[3], region[3]) - region[1],\n",
    "        ]\n",
    "\n",
    "        if new_box[0] >= new_box[2] or new_box[1] >= new_box[3] or area(new_box) < area(orig_box) * 0.5:\n",
    "            continue\n",
    "\n",
    "        for cat_id in annotation['cat_id']:\n",
    "            if (cat_id == None):\n",
    "                continue\n",
    "\n",
    "            category = dataset.get_category(cat_id)\n",
    "            if (category['annotation_set'] != 'deepscores'):\n",
    "                continue\n",
    "\n",
    "            if (category['name'] in {'stem', 'ledgerLine'}):\n",
    "                break\n",
    "            # if (category['name'] in oneset):\n",
    "            labels.append(int(cat_id))\n",
    "            boxes.append(new_box)\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(image[region[1] : region[3], region[0] : region[2]]).div(255).unsqueeze(0),\n",
    "        {\n",
    "            'boxes': torch.tensor(boxes),\n",
    "            'labels': torch.tensor(labels),\n",
    "        }\n",
    "    )\n",
    "\n",
    "def transform(images, targets, dataset):\n",
    "    image_res = []\n",
    "    target_res = []\n",
    "    for image, target in zip(images, targets):\n",
    "        height, width = image.shape\n",
    "\n",
    "        x = random.randrange(0, width // 2)\n",
    "        y = random.randrange(0, height // 2)\n",
    "        region = [x, y, x + width // 2, y + height // 2]\n",
    "\n",
    "        i, t = crop(image, target, region, dataset)\n",
    "\n",
    "        if t['boxes'].shape[0] == 0:\n",
    "            region = [0, 0, width // 2, height // 2]\n",
    "            i, t = crop(image, target, region, dataset)\n",
    "        \n",
    "        image_res.append(i)\n",
    "        target_res.append(t)\n",
    "    return image_res, target_res\n",
    "\n",
    "dataset = MusicSheetDataSet(\"ds2_dense\", \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicSymbolDetector:\n",
    "    def __init__(self):\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn_v2(\n",
    "            pretrained=True,\n",
    "            num_classes=137,\n",
    "            min_size=1024,\n",
    "            max_size=1024,\n",
    "            box_detections_per_img=300\n",
    "        )\n",
    "\n",
    "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
    "        self.optimizer = torch.optim.Adam(params)\n",
    "        self.epoch = 0\n",
    "        self.loss = 0\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        # split image into 6 smaller image for accurate small object detection\n",
    "        height, width = image.shape\n",
    "        box_size = int(width * 0.55)\n",
    "\n",
    "        x_starts = [0, width - box_size]\n",
    "        y_starts = [0, (height - box_size) // 2, height - box_size]\n",
    "\n",
    "        # inference on each sub image\n",
    "        results = []\n",
    "        for x in x_starts:\n",
    "            for y in y_starts:\n",
    "                image = torch.tensor(image[y : y + box_size, x : x + box_size]).unsqueeze(0).to(device)\n",
    "                print(image.shape)\n",
    "                results.append(self.model([image]))\n",
    "        \n",
    "        # combine results, reconstruction\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self):\n",
    "        torch.save({\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"epoch\": self.epoch,\n",
    "            \"loss\": self.loss,\n",
    "        }, f\"fasterrcnn/{self.epoch}\")\n",
    "\n",
    "    def load(self, path = None):\n",
    "        if path == None:\n",
    "            path = self\n",
    "            self = MusicSymbolDetector()\n",
    "        \n",
    "        data = torch.load(path)\n",
    "        self.model.load_state_dict(data['model'])\n",
    "        self.optimizer.load_state_dict(data['optimizer'])\n",
    "        current_epoch = data['epoch']\n",
    "        \n",
    "        print(\"loaded model at epoch: {}, loss: {}\".format(current_epoch, data['loss']))\n",
    "\n",
    "        return self\n",
    "        # move optimizer to cuda\n",
    "        # for state in self.optimizer.state.values():\n",
    "        #     for k, v in state.items():\n",
    "        #         if isinstance(v, torch.Tensor):\n",
    "        #             state[k] = v.cuda()\n",
    "    \n",
    "    def train(self, dataset : MusicSheetDataSet, epochs : int = 1, transform = None):\n",
    "        self.model.to(device)\n",
    "        self.model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        data_count = len(dataset)\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            all_losses = 0\n",
    "            all_losses_dict = {}\n",
    "\n",
    "            dataloader = DataLoader(\n",
    "                dataset,\n",
    "                collate_fn=lambda x : zip(*x),\n",
    "                shuffle = True\n",
    "            )\n",
    "\n",
    "            for images, targets in tqdm(dataloader):\n",
    "                if transform != None:\n",
    "                    images, targets = transform(images, targets, dataset)\n",
    "                \n",
    "                images = [image.to(device) for image in images]\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                loss_dict: dict[str, torch.Tensor] = self.model(images, targets) # the model computes the loss automatically if we pass in targets\n",
    "\n",
    "                losses: torch.Tensor = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                loss_value = losses.item()\n",
    "                all_losses += loss_value\n",
    "                \n",
    "                for k, v in loss_dict.items():\n",
    "                    if k not in all_losses_dict:\n",
    "                        all_losses_dict[k] = 0\n",
    "                    all_losses_dict[k] += v\n",
    "                \n",
    "                if not math.isfinite(loss_value):\n",
    "                    print(f\"Loss is {loss_value}, stopping trainig\") # train if loss becomes infinity\n",
    "                    print(loss_dict)\n",
    "                    sys.exit(1)\n",
    "                \n",
    "                losses.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1)\n",
    "                self.optimizer.step()\n",
    "            \n",
    "            self.epoch += 1\n",
    "            self.loss = all_losses / data_count\n",
    "            print(\"Epoch {:>3}, lr: {:.6f}, loss: {:.6f}, {}\".format(\n",
    "                self.epoch,\n",
    "                self.optimizer.param_groups[0]['lr'], \n",
    "                self.loss,\n",
    "                ', '.join(\"{}: {:.6f}\".format(k, v / data_count) for k, v in all_losses_dict.items()),\n",
    "            ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [08:02<00:00,  2.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1, lr: 0.001000, loss: 1.638613, loss_classifier: 0.533824, loss_box_reg: 0.434572, loss_objectness: 0.164745, loss_rpn_box_reg: 0.505472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [06:10<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2, lr: 0.001000, loss: 1.003286, loss_classifier: 0.230735, loss_box_reg: 0.256892, loss_objectness: 0.109326, loss_rpn_box_reg: 0.406333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [05:45<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3, lr: 0.001000, loss: 0.831465, loss_classifier: 0.167384, loss_box_reg: 0.204791, loss_objectness: 0.108266, loss_rpn_box_reg: 0.351024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [05:40<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4, lr: 0.001000, loss: 0.735072, loss_classifier: 0.143315, loss_box_reg: 0.183961, loss_objectness: 0.080195, loss_rpn_box_reg: 0.327601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [05:40<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5, lr: 0.001000, loss: 0.674543, loss_classifier: 0.122734, loss_box_reg: 0.165570, loss_objectness: 0.075550, loss_rpn_box_reg: 0.310689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [05:42<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6, lr: 0.001000, loss: 0.628249, loss_classifier: 0.114969, loss_box_reg: 0.155037, loss_objectness: 0.068855, loss_rpn_box_reg: 0.289388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [05:45<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7, lr: 0.001000, loss: 0.571289, loss_classifier: 0.102222, loss_box_reg: 0.143025, loss_objectness: 0.058526, loss_rpn_box_reg: 0.267516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [05:41<00:00,  3.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8, lr: 0.001000, loss: 0.534824, loss_classifier: 0.089530, loss_box_reg: 0.133623, loss_objectness: 0.055824, loss_rpn_box_reg: 0.255847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [05:35<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9, lr: 0.001000, loss: 0.517922, loss_classifier: 0.084330, loss_box_reg: 0.128994, loss_objectness: 0.054858, loss_rpn_box_reg: 0.249739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1362/1362 [05:40<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10, lr: 0.001000, loss: 0.490933, loss_classifier: 0.077152, loss_box_reg: 0.121108, loss_objectness: 0.051761, loss_rpn_box_reg: 0.240913\n"
     ]
    }
   ],
   "source": [
    "detector = MusicSymbolDetector()\n",
    "detector.train(dataset, 10, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model at epoch: 10, loss: 0.49093336560018086\n"
     ]
    }
   ],
   "source": [
    "detector = MusicSymbolDetector.load(\"fasterrcnn/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shich\\AppData\\Local\\Temp\\ipykernel_14608\\2330473716.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image[y : y + box_size, x : x + box_size]).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1636, 1636])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67108864 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(img)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# img, res = dataset[266]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# load = torch.load(\"fasterrcnn/1\")\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# model.load_state_dict(load['model'])\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# optimizer.load_state_dict(load['optimizer'])\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# load(\"fasterrcnn/1024-1\")\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m [dataset\u001b[38;5;241m.\u001b[39mget_category(l\u001b[38;5;241m.\u001b[39mitem())[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn [44], line 31\u001b[0m, in \u001b[0;36mMusicSymbolDetector.__call__\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     29\u001b[0m         image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(image[y : y \u001b[38;5;241m+\u001b[39m box_size, x : x \u001b[38;5;241m+\u001b[39m box_size])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 31\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# combine results, reconstruction\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=93'>94</a>\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=94'>95</a>\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=95'>96</a>\u001b[0m                 \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=96'>97</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=97'>98</a>\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=98'>99</a>\u001b[0m             )\n\u001b[1;32m--> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=100'>101</a>\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/generalized_rcnn.py?line=102'>103</a>\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:58\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/backbone_utils.py?line=55'>56</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/backbone_utils.py?line=56'>57</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbody(x)\n\u001b[1;32m---> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/backbone_utils.py?line=57'>58</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfpn(x)\n\u001b[0;32m     <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/models/detection/backbone_utils.py?line=58'>59</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:196\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=193'>194</a>\u001b[0m     inner_top_down \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(last_inner, size\u001b[39m=\u001b[39mfeat_shape, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=194'>195</a>\u001b[0m     last_inner \u001b[39m=\u001b[39m inner_lateral \u001b[39m+\u001b[39m inner_top_down\n\u001b[1;32m--> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=195'>196</a>\u001b[0m     results\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_result_from_layer_blocks(last_inner, idx))\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=197'>198</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextra_blocks \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=198'>199</a>\u001b[0m     results, names \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextra_blocks(results, x, names)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\ops\\feature_pyramid_network.py:169\u001b[0m, in \u001b[0;36mFeaturePyramidNetwork.get_result_from_layer_blocks\u001b[1;34m(self, x, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=166'>167</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_blocks):\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=167'>168</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m idx:\n\u001b[1;32m--> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=168'>169</a>\u001b[0m         out \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torchvision/ops/feature_pyramid_network.py?line=169'>170</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/container.py?line=136'>137</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/container.py?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/container.py?line=138'>139</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/container.py?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1125'>1126</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1126'>1127</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1127'>1128</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1128'>1129</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1129'>1130</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1130'>1131</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/module.py?line=1131'>1132</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/conv.py?line=455'>456</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/conv.py?line=456'>457</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/conv.py?line=448'>449</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/conv.py?line=449'>450</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/conv.py?line=450'>451</a>\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/conv.py?line=451'>452</a>\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/conv.py?line=452'>453</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    <a href='file:///c%3A/Users/shich/AppData/Roaming/Python/Python310/site-packages/torch/nn/modules/conv.py?line=453'>454</a>\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:81] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67108864 bytes."
     ]
    }
   ],
   "source": [
    "# img = cv2.imread(\"ds2_dense/images/lg-900267602436792595-aug-gutenberg1939--page-4.png\", cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.imread(\"sheets/bohemia rhapsody.png\", cv2.IMREAD_GRAYSCALE)\n",
    "img = torch.tensor(img).div(255)\n",
    "\n",
    "# img, res = dataset[266]\n",
    "# load = torch.load(\"fasterrcnn/1\")\n",
    "# model.load_state_dict(load['model'])\n",
    "# optimizer.load_state_dict(load['optimizer'])\n",
    "# load(\"fasterrcnn/1024-1\")\n",
    "\n",
    "res = detector(img)[0]\n",
    "\n",
    "labels = [dataset.get_category(l.item())['name'] for l in res['labels']]\n",
    "\n",
    "print(res['labels'].shape)\n",
    "\n",
    "plt.imshow(draw_bounding_boxes(img.mul(255).type(torch.uint8), res['boxes'], labels).moveaxis(0, 2))\n",
    "plt.savefig(\"img2.png\", dpi=800)\n",
    "\n",
    "\n",
    "# print(img.shape)\n",
    "\n",
    "# device = torch.device(\"cpu\")\n",
    "# results = HRNetBackbbone().to(device)(img[:, 0:1280, 0:1280].unsqueeze(0))\n",
    "# results = model.detect(img)\n",
    "\n",
    "# print(results)\n",
    "\n",
    "# x = results['0'].sum(1).moveaxis(0, 2).detach().cpu()\n",
    "# print(x.shape)\n",
    "# plt.imshow(x)\n",
    "# plt.savefig(\"img.png\", dpi=800)\n",
    "\n",
    "# model.model.backbone = HRNetBackbbone()\n",
    "\n",
    "\n",
    "# TODO: validation, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4148148148148147 1.4138398914518318\n"
     ]
    }
   ],
   "source": [
    "from dataset import MusicSheetDataSet\n",
    "\n",
    "max_asp = 0\n",
    "min_asp = 100\n",
    "\n",
    "def trans(image, target):\n",
    "    global max_asp, min_asp\n",
    "\n",
    "    y, x = image.shape\n",
    "\n",
    "    asp = y / x\n",
    "    if asp > max_asp:\n",
    "        max_asp = asp\n",
    "    if asp < min_asp:\n",
    "        min_asp = asp\n",
    "\n",
    "\n",
    "x = MusicSheetDataSet(\"ds2_dense\", \"train\", trans)\n",
    "\n",
    "for i in x:\n",
    "    pass\n",
    "\n",
    "print(max_asp, min_asp)\n",
    "\n",
    "# image, target = dataset[0]\n",
    "# print(image.shape)\n",
    "# _, x, y = image.shape\n",
    "\n",
    "\n",
    "# scaleX = 800 / x\n",
    "# scaleY = 800 / y\n",
    "\n",
    "# # image = cv2.resize(image.mul(255).type(torch.uint8).numpy(), dsize=(800, 800))\n",
    "\n",
    "# # target = model(image.unsqueeze(0).to(device))[0]\n",
    "\n",
    "# plt.imshow(draw_bounding_boxes(\n",
    "#     image.mul(255).type(torch.uint8), \n",
    "#     torch.concat([torch.tensor([b[0] * scaleX, b[1] * scaleY, b[2] * scaleX, b[3] * scaleY]).unsqueeze(0) for b in target['boxes']]), \n",
    "#     [oneset_rev[x.item()] for x in target['labels']]\n",
    "# ).moveaxis(0, 2))\n",
    "# plt.savefig(\"img.png\", dpi=800)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
